
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Machine Learning Overview &#8212; OpenCV Documentation</title>
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/doxyrest-pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/doxyrest-sphinxdoc.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script type="text/javascript" src="_static/target-highlight.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="OpenCV Tutorials" href="page_tutorial_root.html" />
    <link rel="prev" title="Frequently Asked Questions" href="page_faq.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="page_tutorial_root.html" title="OpenCV Tutorials"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="page_faq.html" title="Frequently Asked Questions"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">OpenCV Documentation</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Machine Learning Overview</a><ul>
<li><a class="reference internal" href="#training-data">Training Data</a></li>
<li><a class="reference internal" href="#normal-bayes-classifier">Normal Bayes Classifier</a></li>
<li><a class="reference internal" href="#k-nearest-neighbors">K-Nearest Neighbors</a></li>
<li><a class="reference internal" href="#support-vector-machines">Support Vector Machines</a><ul>
<li><a class="reference internal" href="#prediction-with-svm">Prediction with SVM</a></li>
</ul>
</li>
<li><a class="reference internal" href="#decision-trees">Decision Trees</a><ul>
<li><a class="reference internal" href="#predicting-with-decision-trees">Predicting with Decision Trees</a></li>
<li><a class="reference internal" href="#training-decision-trees">Training Decision Trees</a></li>
<li><a class="reference internal" href="#variable-importance">Variable Importance</a></li>
</ul>
</li>
<li><a class="reference internal" href="#boosting">Boosting</a><ul>
<li><a class="reference internal" href="#prediction-with-boost">Prediction with Boost</a></li>
</ul>
</li>
<li><a class="reference internal" href="#random-trees">Random Trees</a></li>
<li><a class="reference internal" href="#expectation-maximization">Expectation Maximization</a></li>
<li><a class="reference internal" href="#neural-networks">Neural Networks</a></li>
<li><a class="reference internal" href="#logistic-regression">Logistic Regression</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="page_faq.html"
                        title="previous chapter">Frequently Asked Questions</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="page_tutorial_root.html"
                        title="next chapter">OpenCV Tutorials</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="machine-learning-overview">
<span id="doxid-dc-dd6-ml-intro"></span><span id="index-0"></span><h1>Machine Learning Overview</h1>
<div class="section" id="training-data">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-data"></span><h2>Training Data</h2>
<p>In machine learning algorithms there is notion of training data. Training data includes several components:</p>
<ul class="simple">
<li>A set of training samples. Each training sample is a vector of values (in Computer Vision it’s sometimes referred to as feature vector). Usually all the vectors have the same number of components (features); OpenCV ml module assumes that. Each feature can be ordered (i.e. its values are floating-point numbers that can be compared with each other and strictly ordered, i.e. sorted) or categorical (i.e. its value belongs to a fixed set of values that can be integers, strings etc.).</li>
<li>Optional set of responses corresponding to the samples. Training data with no responses is used in unsupervised learning algorithms that learn structure of the supplied data based on distances between different samples. Training data with responses is used in supervised learning algorithms, which learn the function mapping samples to responses. Usually the responses are scalar values, ordered (when we deal with regression problem) or categorical (when we deal with classification problem; in this case the responses are often called “labels”). Some algorithms, most noticeably Neural networks, can handle not only scalar, but also multi-dimensional or vector responses.</li>
<li>Another optional component is the mask of missing measurements. Most algorithms require all the components in all the training samples be valid, but some other algorithms, such as decision tress, can handle the cases of missing measurements.</li>
<li>In the case of classification problem user may want to give different weights to different classes. This is useful, for example, when:<ul>
<li>user wants to shift prediction accuracy towards lower false-alarm rate or higher hit-rate.</li>
<li>user wants to compensate for significantly different amounts of training samples from different classes.</li>
</ul>
</li>
<li>In addition to that, each training sample may be given a weight, if user wants the algorithm to pay special attention to certain training samples and adjust the training model accordingly.</li>
<li>Also, user may wish not to use the whole training data at once, but rather use parts of it, e.g. to do parameter optimization via cross-validation procedure.</li>
</ul>
<p>As you can see, training data can have rather complex structure; besides, it may be very big and/or not entirely available, so there is need to make abstraction for this concept. In OpenCV ml there is <a class="reference internal" href="class_cv_ml_TrainData.html#doxid-d3-daf-classcv-1-1ml-1-1-train-data"><span class="std std-ref">cv::ml::TrainData</span></a> class for that.</p>
</div>
<div class="section" id="normal-bayes-classifier">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-bayes"></span><h2>Normal Bayes Classifier</h2>
<p>This simple classification model assumes that feature vectors from each class are normally distributed (though, not necessarily independently distributed). So, the whole data distribution function is assumed to be a Gaussian mixture, one component per class. Using the training data the algorithm estimates mean vectors and covariance matrices for every class, and then it uses them for prediction.</p>
</div>
<div class="section" id="k-nearest-neighbors">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-knn"></span><h2>K-Nearest Neighbors</h2>
<p>The algorithm caches all training samples and predicts the response for a new sample by analyzing a certain number (<strong>K</strong>) of the nearest neighbors of the sample using voting, calculating weighted sum, and so on. The method is sometimes referred to as “learning by example” because for prediction it looks for the feature vector with a known response that is closest to the given vector.</p>
</div>
<div class="section" id="support-vector-machines">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-svm"></span><h2>Support Vector Machines</h2>
<p>Originally, support vector machines (SVM) was a technique for building an optimal binary (2-class) classifier. Later the technique was extended to regression and clustering problems. SVM is a partial case of kernel-based methods. It maps feature vectors into a higher-dimensional space using a kernel function and builds an optimal linear discriminating function in this space or an optimal hyper- plane that fits into the training data. In case of SVM, the kernel is not defined explicitly. Instead, a distance between any 2 points in the hyper-space needs to be defined.</p>
<p>The solution is optimal, which means that the margin between the separating hyper-plane and the nearest feature vectors from both classes (in case of 2-class classifier) is maximal. The feature vectors that are the closest to the hyper-plane are called <em>support vectors</em>, which means that the position of other vectors does not affect the hyper-plane (the decision function).</p>
<p>SVM implementation in OpenCV is based on <a class="reference internal" href="page_citelist.html#doxid-d0-de3-citelist-1citeref-libsvm"><span class="std std-ref">[16]</span></a></p>
<div class="section" id="prediction-with-svm">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-svm-predict"></span><h3>Prediction with SVM</h3>
<p>StatModel::predict(samples, results, flags) should be used. Pass flags=StatModel::RAW_OUTPUT to get the raw response from SVM (in the case of regression, 1-class or 2-class classification problem).</p>
</div>
</div>
<div class="section" id="decision-trees">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-trees"></span><h2>Decision Trees</h2>
<p>The ML classes discussed in this section implement Classification and Regression Tree algorithms described in <a class="reference internal" href="page_citelist.html#doxid-d0-de3-citelist-1citeref-breiman84"><span class="std std-ref">[10]</span></a>.</p>
<p>The class <a class="reference internal" href="class_cv_ml_DTrees.html#doxid-d1-d0c-classcv-1-1ml-1-1-d-trees"><span class="std std-ref">cv::ml::DTrees</span></a> represents a single decision tree or a collection of decision trees. It’s also a base class for RTrees and Boost.</p>
<p>A decision tree is a binary tree (tree where each non-leaf node has two child nodes). It can be used either for classification or for regression. For classification, each tree leaf is marked with a class label; multiple leaves may have the same label. For regression, a constant is also assigned to each tree leaf, so the approximation function is piecewise constant.</p>
<div class="section" id="predicting-with-decision-trees">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-trees-predict"></span><h3>Predicting with Decision Trees</h3>
<p>To reach a leaf node and to obtain a response for the input feature vector, the prediction procedure starts with the root node. From each non-leaf node the procedure goes to the left (selects the left child node as the next observed node) or to the right based on the value of a certain variable whose index is stored in the observed node. The following variables are possible:</p>
<ul class="simple">
<li><strong>Ordered variables.</strong> The variable value is compared with a threshold that is also stored in the node. If the value is less than the threshold, the procedure goes to the left. Otherwise, it goes to the right. For example, if the weight is less than 1 kilogram, the procedure goes to the left, else to the right.</li>
<li><strong>Categorical variables.</strong> A discrete variable value is tested to see whether it belongs to a certain subset of values (also stored in the node) from a limited set of values the variable could take. If it does, the procedure goes to the left. Otherwise, it goes to the right. For example, if the color is green or red, go to the left, else to the right.</li>
</ul>
<p>So, in each node, a pair of entities (variable_index , <code class="docutils literal notranslate"><span class="pre">decision_rule</span> <span class="pre">(threshold/subset)</span></code>) is used. This pair is called a <em>split</em> (split on the variable variable_index ). Once a leaf node is reached, the value assigned to this node is used as the output of the prediction procedure.</p>
<p>Sometimes, certain features of the input vector are missed (for example, in the darkness it is difficult to determine the object color), and the prediction procedure may get stuck in the certain node (in the mentioned example, if the node is split by color). To avoid such situations, decision trees use so-called <em>surrogate splits</em>. That is, in addition to the best “primary” split, every tree node may also be split to one or more other variables with nearly the same results.</p>
</div>
<div class="section" id="training-decision-trees">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-trees-train"></span><h3>Training Decision Trees</h3>
<p>The tree is built recursively, starting from the root node. All training data (feature vectors and responses) is used to split the root node. In each node the optimum decision rule (the best “primary” split) is found based on some criteria. In machine learning, gini “purity” criteria are used for classification, and sum of squared errors is used for regression. Then, if necessary, the surrogate splits are found. They resemble the results of the primary split on the training data. All the data is divided using the primary and the surrogate splits (like it is done in the prediction procedure) between the left and the right child node. Then, the procedure recursively splits both left and right nodes. At each node the recursive procedure may stop (that is, stop splitting the node further) in one of the following cases:</p>
<ul class="simple">
<li>Depth of the constructed tree branch has reached the specified maximum value.</li>
<li>Number of training samples in the node is less than the specified threshold when it is not statistically representative to split the node further.</li>
<li>All the samples in the node belong to the same class or, in case of regression, the variation is too small.</li>
<li>The best found split does not give any noticeable improvement compared to a random choice.</li>
</ul>
<p>When the tree is built, it may be pruned using a cross-validation procedure, if necessary. That is, some branches of the tree that may lead to the model overfitting are cut off. Normally, this procedure is only applied to standalone decision trees. Usually tree ensembles build trees that are small enough and use their own protection schemes against overfitting.</p>
</div>
<div class="section" id="variable-importance">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-trees-var"></span><h3>Variable Importance</h3>
<p>Besides the prediction that is an obvious use of decision trees, the tree can be also used for various data analyses. One of the key properties of the constructed decision tree algorithms is an ability to compute the importance (relative decisive power) of each variable. For example, in a spam filter that uses a set of words occurred in the message as a feature vector, the variable importance rating can be used to determine the most “spam-indicating” words and thus help keep the dictionary size reasonable.</p>
<p>Importance of each variable is computed over all the splits on this variable in the tree, primary and surrogate ones. Thus, to compute variable importance correctly, the surrogate splits must be enabled in the training parameters, even if there is no missing data.</p>
</div>
</div>
<div class="section" id="boosting">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-boost"></span><h2>Boosting</h2>
<p>A common machine learning task is supervised learning. In supervised learning, the goal is to learn the functional relationship <span class="math notranslate nohighlight">\(F: y = F(x)\)</span> between the input <span class="math notranslate nohighlight">\(x\)</span> and the output <span class="math notranslate nohighlight">\(y\)</span>. Predicting the qualitative output is called <em>classification</em>, while predicting the quantitative output is called <em>regression</em>.</p>
<p>Boosting is a powerful learning concept that provides a solution to the supervised classification learning task. It combines the performance of many “weak” classifiers to produce a powerful committee <a class="reference internal" href="page_citelist.html#doxid-d0-de3-citelist-1citeref-htf01"><span class="std std-ref">[83]</span></a>. A weak classifier is only required to be better than chance, and thus can be very simple and computationally inexpensive. However, many of them smartly combine results to a strong classifier that often outperforms most “monolithic” strong classifiers such as SVMs and Neural Networks.</p>
<p>Decision trees are the most popular weak classifiers used in boosting schemes. Often the simplest decision trees with only a single split node per tree (called stumps ) are sufficient.</p>
<p>The boosted model is based on <span class="math notranslate nohighlight">\(N\)</span> training examples <span class="math notranslate nohighlight">\({(x_i,y_i)}1N\)</span> with <span class="math notranslate nohighlight">\(x_i \in{R^K}\)</span> and <span class="math notranslate nohighlight">\(y_i \in{-1, +1}\)</span>. <span class="math notranslate nohighlight">\(x_i\)</span> is a <span class="math notranslate nohighlight">\(K\)</span> -component vector. Each component encodes a feature relevant to the learning task at hand. The desired two-class output is encoded as -1 and +1.</p>
<p>Different variants of boosting are known as Discrete Adaboost, Real AdaBoost, LogitBoost, and Gentle AdaBoost <a class="reference internal" href="page_citelist.html#doxid-d0-de3-citelist-1citeref-fht98"><span class="std std-ref">[29]</span></a>. All of them are very similar in their overall structure. Therefore, this chapter focuses only on the standard two-class Discrete AdaBoost algorithm, outlined below. Initially the same weight is assigned to each sample (step 2). Then, a weak classifier <span class="math notranslate nohighlight">\(f_{m(x)}\)</span> is trained on the weighted training data (step 3a). Its weighted training error and scaling factor <span class="math notranslate nohighlight">\(c_m\)</span> is computed (step 3b). The weights are increased for training samples that have been misclassified (step 3c). All weights are then normalized, and the process of finding the next weak classifier continues for another <span class="math notranslate nohighlight">\(M\)</span> -1 times. The final classifier <span class="math notranslate nohighlight">\(F(x)\)</span> is the sign of the weighted sum over the individual weak classifiers (step 4).</p>
<p><strong>Two-class Discrete AdaBoost Algorithm</strong></p>
<ul class="simple">
<li>Set <span class="math notranslate nohighlight">\(N\)</span> examples <span class="math notranslate nohighlight">\({(x_i,y_i)}1N\)</span> with <span class="math notranslate nohighlight">\(x_i \in{R^K}, y_i \in{-1, +1}\)</span>.</li>
<li>Assign weights as <span class="math notranslate nohighlight">\(w_i = 1/N, i = 1,...,N\)</span>.</li>
<li>Repeat for <span class="math notranslate nohighlight">\(m = 1,2,...,M\)</span> :<ul>
<li>Fit the classifier <span class="math notranslate nohighlight">\(f_m(x) \in{-1,1}\)</span>, using weights <span class="math notranslate nohighlight">\(w_i\)</span> on the training data.</li>
<li>Compute <span class="math notranslate nohighlight">\(err_m = E_w [1_{(y \neq f_m(x))}], c_m = log((1 - err_m)/err_m)\)</span>.</li>
<li>Set <span class="math notranslate nohighlight">\(w_i \Leftarrow w_i exp[c_m 1_{(y_i \neq f_m(x_i))}], i = 1,2,...,N,\)</span> and renormalize so that <span class="math notranslate nohighlight">\(\Sigma i w_i = 1\)</span>.</li>
</ul>
</li>
<li>Classify new samples <em>x</em> using the formula: <span class="math notranslate nohighlight">\(\textrm{sign} (\Sigma m = 1M c_m f_m(x))\)</span>.</li>
</ul>
<p>Similar to the classical boosting methods, the current implementation supports two-class classifiers only. For M &gt; 2 classes, there is the <strong>AdaBoost.MH</strong> algorithm (described in <a class="reference internal" href="page_citelist.html#doxid-d0-de3-citelist-1citeref-fht98"><span class="std std-ref">[29]</span></a>) that reduces the problem to the two-class problem, yet with a much larger training set.</p>
<p>To reduce computation time for boosted models without substantially losing accuracy, the influence trimming technique can be employed. As the training algorithm proceeds and the number of trees in the ensemble is increased, a larger number of the training samples are classified correctly and with increasing confidence, thereby those samples receive smaller weights on the subsequent iterations. Examples with a very low relative weight have a small impact on the weak classifier training. Thus, such examples may be excluded during the weak classifier training without having much effect on the induced classifier. This process is controlled with the weight_trim_rate parameter. Only examples with the summary fraction weight_trim_rate of the total weight mass are used in the weak classifier training. Note that the weights for <strong>all</strong> training examples are recomputed at each training iteration. Examples deleted at a particular iteration may be used again for learning some of the weak classifiers further <a class="reference internal" href="page_citelist.html#doxid-d0-de3-citelist-1citeref-fht98"><span class="std std-ref">[29]</span></a></p>
<div class="section" id="prediction-with-boost">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-boost-predict"></span><h3>Prediction with Boost</h3>
<p>StatModel::predict(samples, results, flags) should be used. Pass flags=StatModel::RAW_OUTPUT to get the raw sum from Boost classifier.</p>
</div>
</div>
<div class="section" id="random-trees">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-rtrees"></span><h2>Random Trees</h2>
<p>Random trees have been introduced by Leo Breiman and Adele Cutler: <a class="reference external" href="http://www.stat.berkeley.edu/users/breiman/RandomForests/">http://www.stat.berkeley.edu/users/breiman/RandomForests/</a>. The algorithm can deal with both classification and regression problems. Random trees is a collection (ensemble) of tree predictors that is called <em>forest</em> further in this section (the term has been also introduced by L. Breiman). The classification works as follows: the random trees classifier takes the input feature vector, classifies it with every tree in the forest, and outputs the class label that received the majority of “votes”. In case of a regression, the classifier response is the average of the responses over all the trees in the forest.</p>
<p>All the trees are trained with the same parameters but on different training sets. These sets are generated from the original training set using the bootstrap procedure: for each training set, you randomly select the same number of vectors as in the original set ( =N ). The vectors are chosen with replacement. That is, some vectors will occur more than once and some will be absent. At each node of each trained tree, not all the variables are used to find the best split, but a random subset of them. With each node a new subset is generated. However, its size is fixed for all the nodes and all the trees. It is a training parameter set to <span class="math notranslate nohighlight">\(\sqrt{number\_of\_variables}\)</span> by default. None of the built trees are pruned.</p>
<p>In random trees there is no need for any accuracy estimation procedures, such as cross-validation or bootstrap, or a separate test set to get an estimate of the training error. The error is estimated internally during the training. When the training set for the current tree is drawn by sampling with replacement, some vectors are left out (so-called <em>oob (out-of-bag) data</em>). The size of oob data is about N/3 . The classification error is estimated by using this oob-data as follows:</p>
<ul class="simple">
<li>Get a prediction for each vector, which is oob relative to the i-th tree, using the very i-th tree.</li>
<li>After all the trees have been trained, for each vector that has ever been oob, find the class- <em>winner</em> for it (the class that has got the majority of votes in the trees where the vector was oob) and compare it to the ground-truth response.</li>
<li>Compute the classification error estimate as a ratio of the number of misclassified oob vectors to all the vectors in the original data. In case of regression, the oob-error is computed as the squared error for oob vectors difference divided by the total number of vectors.</li>
</ul>
<p>For the random trees usage example, please, see letter_recog.cpp sample in OpenCV distribution.</p>
<p><strong>References:</strong></p>
<ul class="simple">
<li><em>Machine Learning</em>, Wald I, July 2002. <a class="reference external" href="http://stat-www.berkeley.edu/users/breiman/wald2002-1.pdf">http://stat-www.berkeley.edu/users/breiman/wald2002-1.pdf</a></li>
<li><em>Looking Inside the Black Box</em>, Wald II, July 2002. <a class="reference external" href="http://stat-www.berkeley.edu/users/breiman/wald2002-2.pdf">http://stat-www.berkeley.edu/users/breiman/wald2002-2.pdf</a></li>
<li><em>Software for the Masses</em>, Wald III, July 2002. <a class="reference external" href="http://stat-www.berkeley.edu/users/breiman/wald2002-3.pdf">http://stat-www.berkeley.edu/users/breiman/wald2002-3.pdf</a></li>
<li>And other articles from the web site <a class="reference external" href="http://www.stat.berkeley.edu/users/breiman/RandomForests/cc_home.htm">http://www.stat.berkeley.edu/users/breiman/RandomForests/cc_home.htm</a></li>
</ul>
</div>
<div class="section" id="expectation-maximization">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-em"></span><h2>Expectation Maximization</h2>
<p>The Expectation Maximization(EM) algorithm estimates the parameters of the multivariate probability density function in the form of a Gaussian mixture distribution with a specified number of mixtures.</p>
<p>Consider the set of the N feature vectors {<span class="math notranslate nohighlight">\(x_1, x_2,...,x_{N}\)</span>} from a d-dimensional Euclidean space drawn from a Gaussian mixture:</p>
<div class="math notranslate nohighlight">
\[p(x;a_k,S_k, \pi _k) = \sum _{k=1}^{m} \pi _kp_k(x), \quad \pi _k \geq 0, \quad \sum _{k=1}^{m} \pi _k=1,\]</div>
<div class="math notranslate nohighlight">
\[p_k(x)= \varphi (x;a_k,S_k)= \frac{1}{(2\pi)^{d/2}\mid{S_k}\mid^{1/2}} exp \left \{ - \frac{1}{2} (x-a_k)^TS_k^{-1}(x-a_k) \right \} ,\]</div>
<p>where <span class="math notranslate nohighlight">\(m\)</span> is the number of mixtures, <span class="math notranslate nohighlight">\(p_k\)</span> is the normal distribution density with the mean <span class="math notranslate nohighlight">\(a_k\)</span> and covariance matrix <span class="math notranslate nohighlight">\(S_k\)</span>, <span class="math notranslate nohighlight">\(\pi_k\)</span> is the weight of the k-th mixture. Given the number of mixtures <span class="math notranslate nohighlight">\(M\)</span> and the samples <span class="math notranslate nohighlight">\(x_i\)</span>, <span class="math notranslate nohighlight">\(i=1..N\)</span> the algorithm finds the maximum- likelihood estimates (MLE) of all the mixture parameters, that is, <span class="math notranslate nohighlight">\(a_k\)</span>, <span class="math notranslate nohighlight">\(S_k\)</span> and <span class="math notranslate nohighlight">\(\pi_k\)</span> :</p>
<div class="math notranslate nohighlight">
\[L(x, \theta )=logp(x, \theta )= \sum _{i=1}^{N}log \left ( \sum _{k=1}^{m} \pi _kp_k(x) \right ) \to \max _{ \theta \in \Theta },\]</div>
<div class="math notranslate nohighlight">
\[\Theta = \left \{ (a_k,S_k, \pi _k): a_k \in \mathbbm{R} ^d,S_k=S_k^T&gt;0,S_k \in \mathbbm{R} ^{d \times d}, \pi _k \geq 0, \sum _{k=1}^{m} \pi _k=1 \right \} .\]</div>
<p>The EM algorithm is an iterative procedure. Each iteration includes two steps. At the first step (Expectation step or E-step), you find a probability <span class="math notranslate nohighlight">\(p_{i,k}\)</span> (denoted <span class="math notranslate nohighlight">\(\alpha_{i,k}\)</span> in the formula below) of sample i to belong to mixture k using the currently available mixture parameter estimates:</p>
<div class="math notranslate nohighlight">
\[\alpha _{ki} = \frac{\pi_k\varphi(x;a_k,S_k)}{\sum\limits_{j=1}^{m}\pi_j\varphi(x;a_j,S_j)} .\]</div>
<p>At the second step (Maximization step or M-step), the mixture parameter estimates are refined using the computed probabilities:</p>
<div class="math notranslate nohighlight">
\[\pi _k= \frac{1}{N} \sum _{i=1}^{N} \alpha _{ki}, \quad a_k= \frac{\sum\limits_{i=1}^{N}\alpha_{ki}x_i}{\sum\limits_{i=1}^{N}\alpha_{ki}} , \quad S_k= \frac{\sum\limits_{i=1}^{N}\alpha_{ki}(x_i-a_k)(x_i-a_k)^T}{\sum\limits_{i=1}^{N}\alpha_{ki}}\]</div>
<p>Alternatively, the algorithm may start with the M-step when the initial values for <span class="math notranslate nohighlight">\(p_{i,k}\)</span> can be provided. Another alternative when <span class="math notranslate nohighlight">\(p_{i,k}\)</span> are unknown is to use a simpler clustering algorithm to pre-cluster the input samples and thus obtain initial <span class="math notranslate nohighlight">\(p_{i,k}\)</span>. Often (including machine learning) the k-means algorithm is used for that purpose.</p>
<p>One of the main problems of the EM algorithm is a large number of parameters to estimate. The majority of the parameters reside in covariance matrices, which are <span class="math notranslate nohighlight">\(d \times d\)</span> elements each where <span class="math notranslate nohighlight">\(d\)</span> is the feature space dimensionality. However, in many practical problems, the covariance matrices are close to diagonal or even to <span class="math notranslate nohighlight">\(\mu_k*I\)</span>, where <span class="math notranslate nohighlight">\(I\)</span> is an identity matrix and <span class="math notranslate nohighlight">\(\mu_k\)</span> is a mixture-dependent “scale” parameter. So, a robust computation scheme could start with harder constraints on the covariance matrices and then use the estimated parameters as an input for a less constrained optimization problem (often a diagonal covariance matrix is already a good enough approximation).</p>
<p>References:</p>
<ul class="simple">
<li>Bilmes98 J. A. Bilmes. <em>A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models</em>. Technical Report TR-97-021, International Computer Science Institute and Computer Science Division, University of California at Berkeley, April 1998.</li>
</ul>
</div>
<div class="section" id="neural-networks">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-ann"></span><h2>Neural Networks</h2>
<p>ML implements feed-forward artificial neural networks or, more particularly, multi-layer perceptrons (MLP), the most commonly used type of neural networks. MLP consists of the input layer, output layer, and one or more hidden layers. Each layer of MLP includes one or more neurons directionally linked with the neurons from the previous and the next layer. The example below represents a 3-layer perceptron with three inputs, two outputs, and the hidden layer including five neurons:</p>
<img alt="image" src="_images/mlp.png" />
<p>All the neurons in MLP are similar. Each of them has several input links (it takes the output values from several neurons in the previous layer as input) and several output links (it passes the response to several neurons in the next layer). The values retrieved from the previous layer are summed up with certain weights, individual for each neuron, plus the bias term. The sum is transformed using the activation function <span class="math notranslate nohighlight">\(f\)</span> that may be also different for different neurons.</p>
<img alt="image" src="_images/neuron_model.png" />
<p>In other words, given the outputs <span class="math notranslate nohighlight">\(x_j\)</span> of the layer <span class="math notranslate nohighlight">\(n\)</span>, the outputs <span class="math notranslate nohighlight">\(y_i\)</span> of the layer <span class="math notranslate nohighlight">\(n+1\)</span> are computed as:</p>
<div class="math notranslate nohighlight">
\[u_i = \sum _j (w^{n+1}_{i,j}*x_j) + w^{n+1}_{i,bias}\]</div>
<div class="math notranslate nohighlight">
\[y_i = f(u_i)\]</div>
<p>Different activation functions may be used. ML implements three standard functions:</p>
<ul>
<li><p class="first">Identity function (<a class="reference internal" href="enum_cv_ml_ANN_MLP_ActivationFunctions.html#doxid-db-dd9-classcv-1-1ml-1-1-a-n-n-m-l-p-1ade71470ec8814021728f8b31b09773b0a5cafa9aa38d3f60f8238e867a4a98e0a"><span class="std std-ref">cv::ml::ANN_MLP::IDENTITY</span></a>): <span class="math notranslate nohighlight">\(f(x)=x\)</span></p>
</li>
<li><p class="first">Symmetrical sigmoid (<a class="reference internal" href="enum_cv_ml_ANN_MLP_ActivationFunctions.html#doxid-db-dd9-classcv-1-1ml-1-1-a-n-n-m-l-p-1ade71470ec8814021728f8b31b09773b0a90410002f1e243d35dca234f859f270e"><span class="std std-ref">cv::ml::ANN_MLP::SIGMOID_SYM</span></a>): <span class="math notranslate nohighlight">\(f(x)=\beta*(1-e^{-\alpha x})/(1+e^{-\alpha x}\)</span>), which is the default choice for MLP. The standard sigmoid with <span class="math notranslate nohighlight">\(\beta =1, \alpha =1\)</span> is shown below:</p>
<img alt="image" src="_images/sigmoid_bipolar.png" />
</li>
<li><p class="first">Gaussian function (<a class="reference internal" href="enum_cv_ml_ANN_MLP_ActivationFunctions.html#doxid-db-dd9-classcv-1-1ml-1-1-a-n-n-m-l-p-1ade71470ec8814021728f8b31b09773b0ae3d886f16c8018eebf26d8d75a90dd7e"><span class="std std-ref">cv::ml::ANN_MLP::GAUSSIAN</span></a>): <span class="math notranslate nohighlight">\(f(x)=\beta e^{-\alpha x*x}\)</span>, which is not completely supported at the moment.</p>
</li>
</ul>
<p>In ML, all the neurons have the same activation functions, with the same free parameters (<span class="math notranslate nohighlight">\(\alpha, \beta\)</span>) that are specified by user and are not altered by the training algorithms.</p>
<p>So, the whole trained network works as follows:</p>
<ol class="arabic simple">
<li>Take the feature vector as input. The vector size is equal to the size of the input layer.</li>
<li>Pass values as input to the first hidden layer.</li>
<li>Compute outputs of the hidden layer using the weights and the activation functions.</li>
<li>Pass outputs further downstream until you compute the output layer.</li>
</ol>
<p>So, to compute the network, you need to know all the weights <span class="math notranslate nohighlight">\(w^{n+1)}_{i,j}\)</span>. The weights are computed by the training algorithm. The algorithm takes a training set, multiple input vectors with the corresponding output vectors, and iteratively adjusts the weights to enable the network to give the desired response to the provided input vectors.</p>
<p>The larger the network size (the number of hidden layers and their sizes) is, the more the potential network flexibility is. The error on the training set could be made arbitrarily small. But at the same time the learned network also “learns” the noise present in the training set, so the error on the test set usually starts increasing after the network size reaches a limit. Besides, the larger networks are trained much longer than the smaller ones, so it is reasonable to pre-process the data, using <a class="reference internal" href="class_cv_PCA.html#doxid-d3-d39-classcv-1-1-p-c-a"><span class="std std-ref">cv::PCA</span></a> or similar technique, and train a smaller network on only essential features.</p>
<p>Another MLP feature is an inability to handle categorical data as is. However, there is a workaround. If a certain feature in the input or output (in case of n -class classifier for <span class="math notranslate nohighlight">\(n&gt;2\)</span>) layer is categorical and can take <span class="math notranslate nohighlight">\(M&gt;2\)</span> different values, it makes sense to represent it as a binary tuple of M elements, where the i -th element is 1 if and only if the feature is equal to the i -th value out of M possible. It increases the size of the input/output layer but speeds up the training algorithm convergence and at the same time enables “fuzzy” values of such variables, that is, a tuple of probabilities instead of a fixed value.</p>
<p>ML implements two algorithms for training MLP’s. The first algorithm is a classical random sequential back-propagation algorithm. The second (default) one is a batch RPROP algorithm.</p>
</div>
<div class="section" id="logistic-regression">
<span id="doxid-dc-dd6-ml-intro-1ml-intro-lr"></span><h2>Logistic Regression</h2>
<p>ML implements logistic regression, which is a probabilistic classification technique. Logistic Regression is a binary classification algorithm which is closely related to Support Vector Machines (SVM). Like SVM, Logistic Regression can be extended to work on multi-class classification problems like digit recognition (i.e. recognizing digitis like 0,1 2, 3,… from the given images). This version of Logistic Regression supports both binary and multi-class classifications (for multi-class it creates a multiple 2-class classifiers). In order to train the logistic regression classifier, Batch Gradient Descent and Mini-Batch Gradient Descent algorithms are used (see <a class="reference external" href="http://en.wikipedia.org/wiki/Gradient_descent_optimization">http://en.wikipedia.org/wiki/Gradient_descent_optimization</a>). Logistic Regression is a discriminative classifier (see <a class="reference external" href="http://www.cs.cmu.edu/~tom/NewChapters.html">http://www.cs.cmu.edu/~tom/NewChapters.html</a> for more details). Logistic Regression is implemented as a C++ class in LogisticRegression.</p>
<p>In Logistic Regression, we try to optimize the training paramater <span class="math notranslate nohighlight">\(\theta\)</span> such that the hypothesis <span class="math notranslate nohighlight">\(0 \leq h_\theta(x) \leq 1\)</span> is acheived. We have <span class="math notranslate nohighlight">\(h_\theta(x) = g(h_\theta(x))\)</span> and <span class="math notranslate nohighlight">\(g(z) = \frac{1}{1+e^{-z}}\)</span> as the logistic or sigmoid function. The term “Logistic” in Logistic Regression refers to this function. For given data of a binary classification problem of classes 0 and 1, one can determine that the given data instance belongs to class 1 if <span class="math notranslate nohighlight">\(h_\theta(x) \geq 0.5\)</span> or class 0 if <span class="math notranslate nohighlight">\(h_\theta(x) &lt; 0.5\)</span>.</p>
<p>In Logistic Regression, choosing the right parameters is of utmost importance for reducing the training error and ensuring high training accuracy:</p>
<ul class="simple">
<li>The learning rate can be set with <a class="reference internal" href="class_cv_ml_LogisticRegression.html#doxid-da-d12-classcv-1-1ml-1-1-logistic-regression-1a1d050c28ca1fba9439961f76ba526cec"><span class="std std-ref">setLearningRate</span></a> method. It determines how fast we approach the solution. It is a positive real number.</li>
<li>Optimization algorithms like Batch Gradient Descent and Mini-Batch Gradient Descent are supported in LogisticRegression. It is important that we mention the number of iterations these optimization algorithms have to run. The number of iterations can be set with <a class="reference internal" href="class_cv_ml_LogisticRegression.html#doxid-da-d12-classcv-1-1ml-1-1-logistic-regression-1a77ecde450d828a236b77681f6cd375d8"><span class="std std-ref">setIterations</span></a>. This parameter can be thought as number of steps taken and learning rate specifies if it is a long step or a short step. This and previous parameter define how fast we arrive at a possible solution.</li>
<li>In order to compensate for overfitting regularization is performed, which can be enabled with <a class="reference internal" href="class_cv_ml_LogisticRegression.html#doxid-da-d12-classcv-1-1ml-1-1-logistic-regression-1abfe3f52c9e90a631b37baba63af21a49"><span class="std std-ref">setRegularization</span></a>. One can specify what kind of regularization has to be performed by passing one of <a class="reference internal" href="enum_cv_ml_LogisticRegression_RegKinds.html#doxid-da-d12-classcv-1-1ml-1-1-logistic-regression-1ac3b362508385fbbeffac160e686d9c86"><span class="std std-ref">regularization kinds</span></a> to this method.</li>
<li>Logistic regression implementation provides a choice of 2 training methods with Batch Gradient Descent or the MiniBatch Gradient Descent. To specify this, call <a class="reference internal" href="class_cv_ml_LogisticRegression.html#doxid-da-d12-classcv-1-1ml-1-1-logistic-regression-1a18934b829da316ac02d18d5a413ab221"><span class="std std-ref">setTrainMethod</span></a> with either <a class="reference internal" href="enum_cv_ml_LogisticRegression_Methods.html#doxid-da-d12-classcv-1-1ml-1-1-logistic-regression-1a11394898c4732995dd3af8a665f92894af196a9e88d65d52f36e994b3e3e3a440"><span class="std std-ref">LogisticRegression::BATCH</span></a> or <a class="reference internal" href="enum_cv_ml_LogisticRegression_Methods.html#doxid-da-d12-classcv-1-1ml-1-1-logistic-regression-1a11394898c4732995dd3af8a665f92894aabdf956ee52b567744e53767d0f41df1"><span class="std std-ref">LogisticRegression::MINI_BATCH</span></a>. If training method is set to <a class="reference internal" href="enum_cv_ml_LogisticRegression_Methods.html#doxid-da-d12-classcv-1-1ml-1-1-logistic-regression-1a11394898c4732995dd3af8a665f92894aabdf956ee52b567744e53767d0f41df1"><span class="std std-ref">MINI_BATCH</span></a>, the size of the mini batch has to be to a postive integer set with <a class="reference internal" href="class_cv_ml_LogisticRegression.html#doxid-da-d12-classcv-1-1ml-1-1-logistic-regression-1a1ba3ea00151b9671534e65fe936402b2"><span class="std std-ref">setMiniBatchSize</span></a>.</li>
</ul>
<p>A sample set of training parameters for the Logistic Regression classifier can be initialized as follows:</p>
<pre class="highlight literal-block">
<span></span><span class="n">Ptr</span><span class="o">&lt;</span><span class="n">LogisticRegression</span><span class="o">&gt;</span> <span class="n">lr1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">::</span><span class="n">create</span><span class="p">();</span>
<span class="n">lr1</span><span class="o">-&gt;</span><span class="n">setLearningRate</span><span class="p">(</span><span class="mf">0.001</span><span class="p">);</span>
<span class="n">lr1</span><span class="o">-&gt;</span><span class="n">setIterations</span><span class="p">(</span><span class="mi">10</span><span class="p">);</span>
<span class="n">lr1</span><span class="o">-&gt;</span><span class="n">setRegularization</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="o">::</span><span class="n">REG_L2</span><span class="p">);</span>
<span class="n">lr1</span><span class="o">-&gt;</span><span class="n">setTrainMethod</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="o">::</span><span class="n">BATCH</span><span class="p">);</span>
<span class="n">lr1</span><span class="o">-&gt;</span><span class="n">setMiniBatchSize</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</pre>
<p class="rubric">See also:</p>
<p><a class="reference internal" href="class_cv_ml_TrainData.html#doxid-d3-daf-classcv-1-1ml-1-1-train-data"><span class="std std-ref">cv::ml::TrainData</span></a></p>
<p><a class="reference internal" href="class_cv_ml_NormalBayesClassifier.html#doxid-d0-df9-classcv-1-1ml-1-1-normal-bayes-classifier"><span class="std std-ref">cv::ml::NormalBayesClassifier</span></a></p>
<p><a class="reference internal" href="class_cv_ml_KNearest.html#doxid-d9-dd0-classcv-1-1ml-1-1-k-nearest"><span class="std std-ref">cv::ml::KNearest</span></a></p>
<p><a class="reference internal" href="class_cv_ml_SVM.html#doxid-da-d05-classcv-1-1ml-1-1-s-v-m"><span class="std std-ref">cv::ml::SVM</span></a></p>
<p><a class="reference internal" href="class_cv_ml_DTrees.html#doxid-d1-d0c-classcv-1-1ml-1-1-d-trees"><span class="std std-ref">cv::ml::DTrees</span></a></p>
<p><a class="reference internal" href="class_cv_ml_Boost.html#doxid-d1-ddd-classcv-1-1ml-1-1-boost"><span class="std std-ref">cv::ml::Boost</span></a></p>
<p><a class="reference internal" href="class_cv_ml_RTrees.html#doxid-d4-d41-classcv-1-1ml-1-1-r-trees"><span class="std std-ref">cv::ml::RTrees</span></a></p>
<p><a class="reference internal" href="class_cv_ml_EM.html#doxid-dc-db9-classcv-1-1ml-1-1-e-m"><span class="std std-ref">cv::ml::EM</span></a></p>
<p><a class="reference internal" href="class_cv_ml_ANN_MLP.html#doxid-db-dd9-classcv-1-1ml-1-1-a-n-n-m-l-p"><span class="std std-ref">cv::ml::ANN_MLP</span></a></p>
<p><a class="reference internal" href="class_cv_ml_LogisticRegression.html#doxid-da-d12-classcv-1-1ml-1-1-logistic-regression"><span class="std std-ref">cv::ml::LogisticRegression</span></a></p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="page_tutorial_root.html" title="OpenCV Tutorials"
             >next</a> |</li>
        <li class="right" >
          <a href="page_faq.html" title="Frequently Asked Questions"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">OpenCV Documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 1999-2017, OpenCV Maintainers.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.5.
    </div>
  </body>
</html>