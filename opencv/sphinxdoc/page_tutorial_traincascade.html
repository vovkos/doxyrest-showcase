
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Cascade Classifier Training &#8212; OpenCV Documentation</title>
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/doxyrest-pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/doxyrest-sphinxdoc.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script type="text/javascript" src="_static/target-highlight.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="OpenCV Viz" href="page_tutorial_table_of_content_viz.html" />
    <link rel="prev" title="Cascade Classifier" href="page_tutorial_cascade_classifier.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="page_tutorial_table_of_content_viz.html" title="OpenCV Viz"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="page_tutorial_cascade_classifier.html" title="Cascade Classifier"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">OpenCV Documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="page_tutorial_root.html" >OpenCV Tutorials</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="page_tutorial_table_of_content_objdetect.html" accesskey="U">Object Detection (objdetect module)</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="page_tutorial_cascade_classifier.html"
                        title="previous chapter">Cascade Classifier</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="page_tutorial_table_of_content_viz.html"
                        title="next chapter">OpenCV Viz</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="cascade-classifier-training">
<span id="doxid-dc-d88-tutorial-traincascade"></span><span id="index-0"></span><h1>Cascade Classifier Training</h1>
<p class="rubric">Introduction</p>
<p>Working with a boosted cascade of weak classifiers includes two major stages: the training and the detection stage. The detection stage using either HAAR or LBP based models, is described in the <a class="reference internal" href="page_tutorial_cascade_classifier.html#doxid-db-d28-tutorial-cascade-classifier"><span class="std std-ref">object detection tutorial</span></a>. This documentation gives an overview of the functionality needed to train your own boosted cascade of weak classifiers. The current guide will walk through all the different stages: collecting training data, preparation of the training data and executing the actual model training.</p>
<p>To support this tutorial, several official OpenCV applications will be used: <a class="reference external" href="https://github.com/opencv/opencv/tree/master/apps/createsamples">opencv_createsamples</a>, <a class="reference external" href="https://github.com/opencv/opencv/tree/master/apps/annotation">opencv_annotation</a>, <a class="reference external" href="https://github.com/opencv/opencv/tree/master/apps/traincascade">opencv_traincascade</a> and <a class="reference external" href="https://github.com/opencv/opencv/tree/master/apps/visualisation">opencv_visualisation</a>.</p>
<p class="rubric">Important notes</p>
<ul class="simple">
<li>If you come accross any tutorial mentioning the old opencv_haartraining tool <em>(which is deprecated and still using the OpenCV1.x interface)</em>, then please ignore that tutorial and stick to the opencv_traincascade tool. This tool is a newer version, written in C++ in accordance to the OpenCV 2.x and OpenCV 3.x API. The opencv_traincascade supports both HAAR like wavelet features <a class="reference internal" href="page_citelist.html#doxid-d0-de3-citelist-1citeref-viola01"><span class="std std-ref">[87]</span></a> and LBP (Local Binary Patterns) <a class="reference internal" href="page_citelist.html#doxid-d0-de3-citelist-1citeref-liao2007"><span class="std std-ref">[48]</span></a> features. LBP features yield integer precision in contrast to HAAR features, yielding floating point precision, so both training and detection with LBP are several times faster then with HAAR features. Regarding the LBP and HAAR detection quality, it mainly depends on the training data used and the training parameters selected. It’s possible to train a LBP-based classifier that will provide almost the same quality as HAAR-based one, within a percentage of the training time.</li>
<li>The newer cascade classifier detection interface from OpenCV 2.x and OpenCV 3.x (<a class="reference internal" href="class_cv_CascadeClassifier.html#doxid-dd-d5f-classcv-1-1-cascade-classifier"><span class="std std-ref">cv::CascadeClassifier</span></a>) supports working with both old and new model formats. opencv_traincascade can even save (export) a trained cascade in the older format if for some reason you are stuck using the old interface. At least training the model could then be done in the most stable interface.</li>
<li>The opencv_traincascade application can use TBB for multi-threading. To use it in multicore mode OpenCV must be built with TBB support enabled.</li>
</ul>
<p class="rubric">Preparation of the training data</p>
<p>For training a boosted cascade of weak classifiers we need a set of positive samples (containing actual objects you want to detect) and a set of negative images (containing everything you do not want to detect). The set of negative samples must be prepared manually, whereas set of positive samples is created using the opencv_createsamples application.</p>
<p class="rubric">Negative Samples</p>
<p>Negative samples are taken from arbitrary images, not containing objects you want to detect. These negative images, from which the samples are generated, should be listed in a special negative image file containing one image path per line <em>(can be absolute or relative)</em>. Note that negative samples and sample images are also called background samples or background images, and are used interchangeably in this document.</p>
<p>Described images may be of different sizes. However, each image should be equal or larger than the desired training window size <em>(which corresponds to the model dimensions, most of the times being the average size of your object)</em>, because these images are used to subsample a given negative image into several image samples having this training window size.</p>
<p>An example of such a negative description file:</p>
<p>Directory structure:</p>
<pre class="highlight literal-block">
<span></span><span class="o">/</span><span class="n">img</span>
  <span class="n">img1</span><span class="p">.</span><span class="n">jpg</span>
  <span class="n">img2</span><span class="p">.</span><span class="n">jpg</span>
<span class="n">bg</span><span class="p">.</span><span class="n">txt</span>
</pre>
<p>File bg.txt:</p>
<pre class="highlight literal-block">
<span></span><span class="n">img</span><span class="o">/</span><span class="n">img1</span><span class="p">.</span><span class="n">jpg</span>
<span class="n">img</span><span class="o">/</span><span class="n">img2</span><span class="p">.</span><span class="n">jpg</span>
</pre>
<p>Your set of negative window samples will be used to tell the machine learning step, boosting in this case, what not to look for, when trying to find your objects of interest.</p>
<p class="rubric">Positive Samples</p>
<p>Positive samples are created by the opencv_createsamples application. They are used by the boosting process to define what the model should actually look for when trying to find your objects of interest. The application supports two ways of generating a positive sample dataset.</p>
<ol class="arabic simple">
<li>You can generate a bunch of positives from a single positive object image.</li>
<li>You can supply all the positives yourself and only use the tool to cut them out, resize them and put them in the opencv needed binary format.</li>
</ol>
<p>While the first approach works decently for fixed objects, like very rigid logo’s, it tends to fail rather soon for less rigid objects. In that case we do suggest to use the second approach. Many tutorials on the web even state that 100 real object images, can lead to a better model than 1000 artificially generated positives, by using the opencv_createsamples application. If you however do decide to take the first approach, keep some things in mind:</p>
<ul class="simple">
<li>Please note that you need more than a single positive samples before you give it to the mentioned application, because it only applies perspective transformation.</li>
<li>If you want a robust model, take samples that cover the wide range of varieties that can occur within your object class. For example in the case of faces you should consider different races and age groups, emotions and perhaps beard styles. This also applies when using the second approach.</li>
</ul>
<p>The first approach takes a single object image with for example a company logo and creates a large set of positive samples from the given object image by randomly rotating the object, changing the image intensity as well as placing the image on arbitrary backgrounds. The amount and range of randomness can be controlled by command line arguments of the opencv_createsamples application.</p>
<p>Command line arguments:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">-vec</span> <span class="pre">&lt;vec_file_name&gt;</span></code> : Name of the output file containing the positive samples for training.</li>
<li><code class="docutils literal notranslate"><span class="pre">-img</span> <span class="pre">&lt;image_file_name&gt;</span></code> : Source object image (e.g., a company logo).</li>
<li><code class="docutils literal notranslate"><span class="pre">-bg</span> <span class="pre">&lt;background_file_name&gt;</span></code> : Background description file; contains a list of images which are used as a background for randomly distorted versions of the object.</li>
<li><code class="docutils literal notranslate"><span class="pre">-num</span> <span class="pre">&lt;number_of_samples&gt;</span></code> : Number of positive samples to generate.</li>
<li><code class="docutils literal notranslate"><span class="pre">-bgcolor</span> <span class="pre">&lt;background_color&gt;</span></code> : Background color (currently grayscale images are assumed); the background color denotes the transparent color. Since there might be compression artifacts, the amount of color tolerance can be specified by -bgthresh. All pixels withing bgcolor-bgthresh and bgcolor+bgthresh range are interpreted as transparent.</li>
<li><code class="docutils literal notranslate"><span class="pre">-bgthresh</span> <span class="pre">&lt;background_color_threshold&gt;</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">-inv</span></code> : If specified, colors will be inverted.</li>
<li><code class="docutils literal notranslate"><span class="pre">-randinv</span></code> : If specified, colors will be inverted randomly.</li>
<li><code class="docutils literal notranslate"><span class="pre">-maxidev</span> <span class="pre">&lt;max_intensity_deviation&gt;</span></code> : Maximal intensity deviation of pixels in foreground samples.</li>
<li><code class="docutils literal notranslate"><span class="pre">-maxxangle</span> <span class="pre">&lt;max_x_rotation_angle&gt;</span></code> : Maximal rotation angle towards x-axis, must be given in radians.</li>
<li><code class="docutils literal notranslate"><span class="pre">-maxyangle</span> <span class="pre">&lt;max_y_rotation_angle&gt;</span></code> : Maximal rotation angle towards y-axis, must be given in radians.</li>
<li><code class="docutils literal notranslate"><span class="pre">-maxzangle</span> <span class="pre">&lt;max_z_rotation_angle&gt;</span></code> : Maximal rotation angle towards z-axis, must be given in radians.</li>
<li><code class="docutils literal notranslate"><span class="pre">-show</span></code> : Useful debugging option. If specified, each sample will be shown. Pressing Esc will continue the samples creation process without showing each sample.</li>
<li><code class="docutils literal notranslate"><span class="pre">-w</span> <span class="pre">&lt;sample_width&gt;</span></code> : Width (in pixels) of the output samples.</li>
<li><code class="docutils literal notranslate"><span class="pre">-h</span> <span class="pre">&lt;sample_height&gt;</span></code> : Height (in pixels) of the output samples.</li>
</ul>
<p>When running opencv_createsamples in this way, the following procedure is used to create a sample object instance: The given source image is rotated randomly around all three axes. The chosen angle is limited by <code class="docutils literal notranslate"><span class="pre">-maxxangle</span></code>, <code class="docutils literal notranslate"><span class="pre">-maxyangle</span></code> and <code class="docutils literal notranslate"><span class="pre">-maxzangle</span></code>. Then pixels having the intensity from the [bg_color-bg_color_threshold; bg_color+bg_color_threshold] range are interpreted as transparent. White noise is added to the intensities of the foreground. If the <code class="docutils literal notranslate"><span class="pre">-inv</span></code> key is specified then foreground pixel intensities are inverted. If <code class="docutils literal notranslate"><span class="pre">-randinv</span></code> key is specified then algorithm randomly selects whether inversion should be applied to this sample. Finally, the obtained image is placed onto an arbitrary background from the background description file, resized to the desired size specified by <code class="docutils literal notranslate"><span class="pre">-w</span></code> and <code class="docutils literal notranslate"><span class="pre">-h</span></code> and stored to the vec-file, specified by the <code class="docutils literal notranslate"><span class="pre">-vec</span></code> command line option.</p>
<p>Positive samples also may be obtained from a collection of previously marked up images, which is the desired way when building robust object models. This collection is described by a text file similar to the background description file. Each line of this file corresponds to an image. The first element of the line is the filename, followed by the number of object annotations, followed by numbers describing the coordinates of the objects bounding rectangles (x, y, width, height).</p>
<p>An example of description file:</p>
<p>Directory structure:</p>
<pre class="highlight literal-block">
<span></span><span class="o">/</span><span class="n">img</span>
  <span class="n">img1</span><span class="p">.</span><span class="n">jpg</span>
  <span class="n">img2</span><span class="p">.</span><span class="n">jpg</span>
<span class="n">info</span><span class="p">.</span><span class="n">dat</span>
</pre>
<p>File info.dat:</p>
<pre class="highlight literal-block">
<span></span><span class="n">img</span><span class="o">/</span><span class="n">img1</span><span class="p">.</span><span class="n">jpg</span>  <span class="mi">1</span>  <span class="mi">140</span> <span class="mi">100</span> <span class="mi">45</span> <span class="mi">45</span>
<span class="n">img</span><span class="o">/</span><span class="n">img2</span><span class="p">.</span><span class="n">jpg</span>  <span class="mi">2</span>  <span class="mi">100</span> <span class="mi">200</span> <span class="mi">50</span> <span class="mi">50</span>   <span class="mi">50</span> <span class="mi">30</span> <span class="mi">25</span> <span class="mi">25</span>
</pre>
<p>Image img1.jpg contains single object instance with the following coordinates of bounding rectangle: (140, 100, 45, 45). Image img2.jpg contains two object instances.</p>
<p>In order to create positive samples from such collection, <code class="docutils literal notranslate"><span class="pre">-info</span></code> argument should be specified instead of <code class="docutils literal notranslate"><span class="pre">-img</span></code> :</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">-info</span> <span class="pre">&lt;collection_file_name&gt;</span></code> : Description file of marked up images collection.</li>
</ul>
<p>Note that in this case, parameters like <code class="docutils literal notranslate"><span class="pre">-bg,</span> <span class="pre">-bgcolor,</span> <span class="pre">-bgthreshold,</span> <span class="pre">-inv,</span> <span class="pre">-randinv,</span> <span class="pre">-maxxangle,</span> <span class="pre">-maxyangle,</span> <span class="pre">-maxzangle</span></code> are simply ignored and not used anymore. The scheme of samples creation in this case is as follows. The object instances are taken from the given images, by cutting out the supplied bounding boxes from the original images. Then they are resized to target samples size (defined by <code class="docutils literal notranslate"><span class="pre">-w</span></code> and <code class="docutils literal notranslate"><span class="pre">-h</span></code>) and stored in output vec-file, defined by the <code class="docutils literal notranslate"><span class="pre">-vec</span></code> parameter. No distortion is applied, so the only affecting arguments are <code class="docutils literal notranslate"><span class="pre">-w</span></code>, <code class="docutils literal notranslate"><span class="pre">-h</span></code>, <code class="docutils literal notranslate"><span class="pre">-show</span></code> and <code class="docutils literal notranslate"><span class="pre">-num</span></code>.</p>
<p>The manual process of creating the <code class="docutils literal notranslate"><span class="pre">-info</span></code> file can also been done by using the opencv_annotation tool. This is an open source tool for visually selecting the regions of interest of your object instances in any given images. The following subsection will discuss in more detail on how to use this application.</p>
<p class="rubric">Extra remarks</p>
<ul class="simple">
<li>opencv_createsamples utility may be used for examining samples stored in any given positive samples file. In order to do this only <code class="docutils literal notranslate"><span class="pre">-vec</span></code>, <code class="docutils literal notranslate"><span class="pre">-w</span></code> and <code class="docutils literal notranslate"><span class="pre">-h</span></code> parameters should be specified.</li>
<li>Example of vec-file is available here <code class="docutils literal notranslate"><span class="pre">opencv/data/vec_files/trainingfaces_24-24.vec</span></code>. It can be used to train a face detector with the following window size: <code class="docutils literal notranslate"><span class="pre">-w</span> <span class="pre">24</span> <span class="pre">-h</span> <span class="pre">24</span></code>.</li>
</ul>
<p class="rubric">Using OpenCV’s integrated annotation tool</p>
<p>Since OpenCV 3.x the community has been supplying and maintaining a open source annotation tool, used for generating the <code class="docutils literal notranslate"><span class="pre">-info</span></code> file. The tool can be accessed by the command opencv_annotation if the OpenCV applications where build.</p>
<p>Using the tool is quite straightforward. The tool accepts several required and some optional parameters:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">--annotations</span></code> <strong>(required)</strong> : path to annotations txt file, where you want to store your annotations, which is then passed to the <code class="docutils literal notranslate"><span class="pre">-info</span></code> parameter [example - /data/annotations.txt]</li>
<li><code class="docutils literal notranslate"><span class="pre">--images</span></code> <strong>(required)</strong> : path to folder containing the images with your objects [example - /data/testimages/]</li>
<li><code class="docutils literal notranslate"><span class="pre">--maxWindowHeight</span></code> <em>(optional)</em> : if the input image is larger in height then the given resolution here, resize the image for easier annotation, using <code class="docutils literal notranslate"><span class="pre">--resizeFactor</span></code>.</li>
<li><code class="docutils literal notranslate"><span class="pre">--resizeFactor</span></code> <em>(optional)</em> : factor used to resize the input image when using the <code class="docutils literal notranslate"><span class="pre">--maxWindowHeight</span></code> parameter.</li>
</ul>
<p>Note that the optional parameters can only be used together. An example of a command that could be used can be seen below</p>
<pre class="highlight literal-block">
<span></span><span class="n">opencv_annotation</span> <span class="o">--</span><span class="n">annotations</span><span class="o">=/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">annotations</span><span class="o">/</span><span class="n">file</span><span class="p">.</span><span class="n">txt</span> <span class="o">--</span><span class="n">images</span><span class="o">=/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">image</span><span class="o">/</span><span class="n">folder</span><span class="o">/</span>
</pre>
<p>This command will fire up a window containing the first image and your mouse cursor which will be used for annotation. A video on how to use the annotation tool can be found <a class="reference external" href="https://www.youtube.com/watch?v=EV5gmvoCTSk">here</a>. Basically there are several keystrokes that trigger an action. The left mouse button is used to select the first corner of your object, then keeps drawing until you are fine, and stops when a second left mouse button click is registered. After each selection you have the following choices:</p>
<ul class="simple">
<li>Pressing <code class="docutils literal notranslate"><span class="pre">c</span></code> : confirm the annotation, turning the annotation green and confirming it is stored</li>
<li>Pressing <code class="docutils literal notranslate"><span class="pre">d</span></code> : delete the last annotation from the list of annotations (easy for removing wrong annotations)</li>
<li>Pressing <code class="docutils literal notranslate"><span class="pre">n</span></code> : continue to the next image</li>
<li>Pressing <code class="docutils literal notranslate"><span class="pre">ESC</span></code> : this will exit the annotation software</li>
</ul>
<p>Finally you will end up with a usable annotation file that can be passed to the <code class="docutils literal notranslate"><span class="pre">-info</span></code> argument of opencv_createsamples.</p>
<p class="rubric">Cascade Training</p>
<p>The next step is the actual training of the boosted cascade of weak classifiers, based on the positive and negative dataset that was prepared beforehand.</p>
<p>Command line arguments of opencv_traincascade application grouped by purposes:</p>
<ul class="simple">
<li>Common arguments:<ul>
<li><code class="docutils literal notranslate"><span class="pre">-data</span> <span class="pre">&lt;cascade_dir_name&gt;</span></code> : Where the trained classifier should be stored. This folder should be created manually beforehand.</li>
<li><code class="docutils literal notranslate"><span class="pre">-vec</span> <span class="pre">&lt;vec_file_name&gt;</span></code> : vec-file with positive samples (created by opencv_createsamples utility).</li>
<li><code class="docutils literal notranslate"><span class="pre">-bg</span> <span class="pre">&lt;background_file_name&gt;</span></code> : Background description file. This is the file containing the negative sample images.</li>
<li><code class="docutils literal notranslate"><span class="pre">-numPos</span> <span class="pre">&lt;number_of_positive_samples&gt;</span></code> : Number of positive samples used in training for every classifier stage.</li>
<li><code class="docutils literal notranslate"><span class="pre">-numNeg</span> <span class="pre">&lt;number_of_negative_samples&gt;</span></code> : Number of negative samples used in training for every classifier stage.</li>
<li><code class="docutils literal notranslate"><span class="pre">-numStages</span> <span class="pre">&lt;number_of_stages&gt;</span></code> : Number of cascade stages to be trained.</li>
<li><code class="docutils literal notranslate"><span class="pre">-precalcValBufSize</span> <span class="pre">&lt;precalculated_vals_buffer_size_in_Mb&gt;</span></code> : Size of buffer for precalculated feature values (in Mb). The more memory you assign the faster the training process, however keep in mind that <code class="docutils literal notranslate"><span class="pre">-precalcValBufSize</span></code> and <code class="docutils literal notranslate"><span class="pre">-precalcIdxBufSize</span></code> combined should not exceed you available system memory.</li>
<li><code class="docutils literal notranslate"><span class="pre">-precalcIdxBufSize</span> <span class="pre">&lt;precalculated_idxs_buffer_size_in_Mb&gt;</span></code> : Size of buffer for precalculated feature indices (in Mb). The more memory you assign the faster the training process, however keep in mind that <code class="docutils literal notranslate"><span class="pre">-precalcValBufSize</span></code> and <code class="docutils literal notranslate"><span class="pre">-precalcIdxBufSize</span></code> combined should not exceed you available system memory.</li>
<li><code class="docutils literal notranslate"><span class="pre">-baseFormatSave</span></code> : This argument is actual in case of Haar-like features. If it is specified, the cascade will be saved in the old format. This is only available for backwards compatibility reasons and to allow users stuck to the old deprecated interface, to at least train models using the newer interface.</li>
<li><code class="docutils literal notranslate"><span class="pre">-numThreads</span> <span class="pre">&lt;max_number_of_threads&gt;</span></code> : Maximum number of threads to use during training. Notice that the actual number of used threads may be lower, depending on your machine and compilation options. By default, the maximum available threads are selected if you built OpenCV with TBB support, which is needed for this optimization.</li>
<li><code class="docutils literal notranslate"><span class="pre">-acceptanceRatioBreakValue</span> <span class="pre">&lt;break_value&gt;</span></code> : This argument is used to determine how precise your model should keep learning and when to stop. A good guideline is to train not further than 10e-5, to ensure the model does not overtrain on your training data. By default this value is set to -1 to disable this feature.</li>
</ul>
</li>
<li>Cascade parameters:<ul>
<li><code class="docutils literal notranslate"><span class="pre">-stageType</span> <span class="pre">&lt;BOOST(default)&gt;</span></code> : Type of stages. Only boosted classifiers are supported as a stage type at the moment.</li>
<li><code class="docutils literal notranslate"><span class="pre">-featureType&lt;{HAAR(default),</span> <span class="pre">LBP}&gt;</span></code> : Type of features: HAAR - Haar-like features, LBP - local binary patterns.</li>
<li><code class="docutils literal notranslate"><span class="pre">-w</span> <span class="pre">&lt;sampleWidth&gt;</span></code> : Width of training samples (in pixels). Must have exactly the same value as used during training samples creation (opencv_createsamples utility).</li>
<li><code class="docutils literal notranslate"><span class="pre">-h</span> <span class="pre">&lt;sampleHeight&gt;</span></code> : Height of training samples (in pixels). Must have exactly the same value as used during training samples creation (opencv_createsamples utility).</li>
</ul>
</li>
<li>Boosted classifer parameters:<ul>
<li><code class="docutils literal notranslate"><span class="pre">-bt</span> <span class="pre">&lt;{DAB,</span> <span class="pre">RAB,</span> <span class="pre">LB,</span> <span class="pre">GAB(default)}&gt;</span></code> : Type of boosted classifiers: DAB - Discrete AdaBoost, RAB - Real AdaBoost, LB - LogitBoost, GAB - Gentle AdaBoost.</li>
<li><code class="docutils literal notranslate"><span class="pre">-minHitRate</span> <span class="pre">&lt;min_hit_rate&gt;</span></code> : Minimal desired hit rate for each stage of the classifier. Overall hit rate may be estimated as (min_hit_rate ^ number_of_stages), <a class="reference internal" href="page_citelist.html#doxid-d0-de3-citelist-1citeref-viola04"><span class="std std-ref">[88]</span></a> §4.1.</li>
<li><code class="docutils literal notranslate"><span class="pre">-maxFalseAlarmRate</span> <span class="pre">&lt;max_false_alarm_rate&gt;</span></code> : Maximal desired false alarm rate for each stage of the classifier. Overall false alarm rate may be estimated as (max_false_alarm_rate ^ number_of_stages), <a class="reference internal" href="page_citelist.html#doxid-d0-de3-citelist-1citeref-viola04"><span class="std std-ref">[88]</span></a> §4.1.</li>
<li><code class="docutils literal notranslate"><span class="pre">-weightTrimRate</span> <span class="pre">&lt;weight_trim_rate&gt;</span></code> : Specifies whether trimming should be used and its weight. A decent choice is 0.95.</li>
<li><code class="docutils literal notranslate"><span class="pre">-maxDepth</span> <span class="pre">&lt;max_depth_of_weak_tree&gt;</span></code> : Maximal depth of a weak tree. A decent choice is 1, that is case of stumps.</li>
<li><code class="docutils literal notranslate"><span class="pre">-maxWeakCount</span> <span class="pre">&lt;max_weak_tree_count&gt;</span></code> : Maximal count of weak trees for every cascade stage. The boosted classifier (stage) will have so many weak trees (&lt;=maxWeakCount), as needed to achieve the given <code class="docutils literal notranslate"><span class="pre">-maxFalseAlarmRate</span></code>.</li>
</ul>
</li>
<li>Haar-like feature parameters:<ul>
<li><code class="docutils literal notranslate"><span class="pre">-mode</span> <span class="pre">&lt;BASIC</span> <span class="pre">(default)</span> <span class="pre">|</span> <span class="pre">CORE</span> <span class="pre">|</span> <span class="pre">ALL&gt;</span></code> : Selects the type of Haar features set used in training. BASIC use only upright features, while ALL uses the full set of upright and 45 degree rotated feature set. See <a class="reference internal" href="page_citelist.html#doxid-d0-de3-citelist-1citeref-lienhart02"><span class="std std-ref">[49]</span></a> for more details.</li>
</ul>
</li>
<li>Local Binary Patterns parameters: Local Binary Patterns don’t have parameters.</li>
</ul>
<p>After the opencv_traincascade application has finished its work, the trained cascade will be saved in <code class="docutils literal notranslate"><span class="pre">cascade.xml</span></code> file in the <code class="docutils literal notranslate"><span class="pre">-data</span></code> folder. Other files in this folder are created for the case of interrupted training, so you may delete them after completion of training.</p>
<p>Training is finished and you can test your cascade classifier!</p>
<p class="rubric">Visualising Cascade Classifiers</p>
<p>From time to time it can be usefull to visualise the trained cascade, to see which features it selected and how complex its stages are. For this OpenCV supplies a opencv_visualisation application. This application has the following commands:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">--image</span></code> <strong>(required)</strong> : path to a reference image for your object model. This should be an annotation with dimensions [<code class="docutils literal notranslate"><span class="pre">-w</span></code>, <code class="docutils literal notranslate"><span class="pre">-h</span></code>] as passed to both opencv_createsamples and opencv_traincascade application.</li>
<li><code class="docutils literal notranslate"><span class="pre">--model</span></code> <strong>(required)</strong> : path to the trained model, which should be in the folder supplied to the <code class="docutils literal notranslate"><span class="pre">-data</span></code> parameter of the opencv_traincascade application.</li>
<li><code class="docutils literal notranslate"><span class="pre">--data</span></code> <em>(optional)</em> : if a data folder is supplied, which has to be manually created beforehand, stage output and a video of the features will be stored.</li>
</ul>
<p>An example command can be seen below</p>
<pre class="highlight literal-block">
<span></span><span class="n">opencv_visualisation</span> <span class="o">--</span><span class="n">image</span><span class="o">=/</span><span class="n">data</span><span class="o">/</span><span class="n">object</span><span class="p">.</span><span class="n">png</span> <span class="o">--</span><span class="n">model</span><span class="o">=/</span><span class="n">data</span><span class="o">/</span><span class="n">model</span><span class="p">.</span><span class="n">xml</span> <span class="o">--</span><span class="n">data</span><span class="o">=/</span><span class="n">data</span><span class="o">/</span><span class="n">result</span><span class="o">/</span>
</pre>
<p>Some limitations of the current visualisation tool</p>
<ul class="simple">
<li>Only handles cascade classifier models, trained with the opencv_traincascade tool, containing <strong>stumps</strong> as decision trees [default settings].</li>
<li>The image provided needs to be a sample window with the original model dimensions, passed to the <code class="docutils literal notranslate"><span class="pre">--image</span></code> parameter.</li>
</ul>
<p>Example of the HAAR/LBP face model ran on a given window of Angelina Jolie, which had the same preprocessing as cascade classifier files&gt;24x24 pixel image, grayscale conversion and histogram equalisation:</p>
<p><em>A video is made with for each stage each feature visualised:</em></p>
<img alt="_images/visualisation_video.png" src="_images/visualisation_video.png" />
<p><em>Each stage is stored as an image for future validation of the features:</em></p>
<img alt="_images/visualisation_single_stage.png" src="_images/visualisation_single_stage.png" />
<p>This work was created for <a class="reference external" href="https://www.packtpub.com/application-development/opencv-3-blueprints">OpenCV 3 Blueprints</a> by StevenPuttemans but Packt Publishing agreed integration into OpenCV.</p>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="page_tutorial_table_of_content_viz.html" title="OpenCV Viz"
             >next</a> |</li>
        <li class="right" >
          <a href="page_tutorial_cascade_classifier.html" title="Cascade Classifier"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">OpenCV Documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="page_tutorial_root.html" >OpenCV Tutorials</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="page_tutorial_table_of_content_objdetect.html" >Object Detection (objdetect module)</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 1999-2017, OpenCV Maintainers.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.5.
    </div>
  </body>
</html>